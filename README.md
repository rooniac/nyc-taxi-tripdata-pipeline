# ğŸš• NYC Taxi Trip Data Engineering Pipeline

An end-to-end Data Engineering project that builds a scalable, modular data pipeline for NYC Yellow and Green Taxi trip data. The system support both **batch** and **streaming** ingestion, applies transformations with **Spark** and **dbt**, stores results in a **Data Lake + Data Warehouse**, and visualize sample insights with **Superset**.

ğŸš§ **This project is still under active development. Features and structure may change.**

## ğŸ“– Table of Contents

1. [ğŸ“Œ Project Overview](#-project-overview)
2. [ğŸ§¬ Data Flow Architecture](#-data-flow-architecture)
3. [ğŸ“ Project Folder Structure](#-project-folder-structure)
4. [ğŸ§ª Get Started](#-get-started)
   - [âš™ï¸ Batch Processing](#ï¸-batch-processing)
   - [ğŸ“¡ Streaming Processing](#-streaming-processing)
   - [ğŸ” dbt for Data Warehouse](#-dbt-for-data-warehouse)
5. [â° Airflow DAGs Orchestration](#-airflow-dags-orchestration)
6. [ğŸ“Š BI Visualization with Superset](#-bi-visualization-with-superset)
7. [ğŸ“š References](#-references)

## ğŸ“Œ Project Overview

This project simulates a real-world Data Engineering pipeline for NYC taxi trips using open-source tools. It demonstrates key data engineering concepts end-to-end, including:

- Batch processing with Parquet files
- Streaming ingestion using Kafka and Debezium (CDC)
- Data Lake architecture with raw / processed / sandbox zones
- ETL with Spark and Delta Lake
- Data transformation with dbt
- Pipeline orchestration using Apache Airflow
- Business Intelligence dashboarding using Superset

### ğŸ“¦ Tech Stack

- **Language**: Python 3.9.7
- **Batch Processing**: Apache Spark 3.5.6, Delta Lake
- **Streaming Ingestion**: Apache Kafka, Debezium (CDC), Spark Structured Streaming
- **Orchestration**: Apache Airflow
- **Object Storage**: MinIO (S3-compatible)
- **Data Lake Format**: Delta Lake + Hive Metastore
- **Data Warehouse**: PostgreSQL (with staging and production schemas)
- **Transformation Layer**: dbt (Data Build Tool)
- **BI & Visualization**: Apache Superset
- **Containerization**: Docker

## ğŸ§¬ Data Flow Architecture

![Data Flow Architecture](./images/dataflow.png)

<p align="center">
    <em>Data Flow Architecture</em>
</p>

The architecture includes:

- Batch ingestion from NYC Trip Record Data `.parquet` files
- Streaming ingestion via CDC (Debezium -> Kafka -> Spark Streaming)
- Spark batch jobs to clean and load data from raw -> processed -> sandbox, processed -> staging (PostgreSQL Datawarehouse)
- dbt for transforming staging data to production (star schema)
- Superset for dashboarding

## ğŸ“ Project Folder Structure

```bash
.
    â”œâ”€â”€ airflow/                                    /* airflow folder /*
    â”œâ”€â”€ configs/
    â”‚   â””â”€â”€ datalake.yaml                           /* config of MinIO /*
    â”œâ”€â”€ data/                                       /* contain dataset /*
    â”‚   â”œâ”€â”€ 2023/
    â”‚   â”œâ”€â”€ 2024/
    â”‚   â””â”€â”€ download_data.py
    â”œâ”€â”€ dbt_tripdata/                               /* dbt folder for transformation datawarehouse /*
    â”œâ”€â”€ debezium/
    â”‚    â””â”€â”€ configs/
    â”‚       â””â”€â”€  taxi-nyc-cdc-json                  /* config file to setup connection debezium cdc with kafka /*
    â”œâ”€â”€ docker/
    â”‚   â”œâ”€â”€ docker-compose.yaml
    â”‚   â”œâ”€â”€ airflow-docker-compose.yaml
    â”‚   â”œâ”€â”€ postgres-docker-compose.yaml
    â”‚   â”œâ”€â”€ stream-docker-compose.yaml
    â”‚   â””â”€â”€ superset-docker-compose.yaml
    â”œâ”€â”€ pipelines/                                  /* spark jobs for pipeline /*
    â”‚   â”œâ”€â”€ data/
    â”‚       â””â”€â”€ taxi_zone_lookup.csv
    â”‚   â”œâ”€â”€ datalake_to_dw/
    â”‚       â””â”€â”€ processed_to_staging.py
    â”‚   â”œâ”€â”€ delta/
    â”‚       â””â”€â”€ convert_to_delta.py
    â”‚   â”œâ”€â”€ ingest/
    â”‚       â””â”€â”€ ingest_to_raw.py
    â”‚   â””â”€â”€ transform/
    â”‚       â””â”€â”€ transform_data.py
    â”œâ”€â”€ scripts/
    â”‚   â”œâ”€â”€ create_cdcsource_table_pg.py
    â”‚   â”œâ”€â”€ create_schema_postgres.py
    â”‚   â”œâ”€â”€ create_staging_table_pg.py
    â”‚   â””â”€â”€ insert_to_cdc_source.py
    â”œâ”€â”€ imgs/
    â”œâ”€â”€ jars/                                       /* JAR files for Spark*/
    â”œâ”€â”€ streaming_processing/
    â”‚    â””â”€â”€ streaming_to_datalake.py               /* read data streaming from kafka and push into raw zone MinIO */
    â”œâ”€â”€ utils/
    â”‚    â”œâ”€â”€ postgresql_client.py                   /* PostgreSQL Client: create connect, execute query, get columns in bucket /*
    â”‚    â”œâ”€â”€ helper.py
    â”‚    â””â”€â”€ minio_utils.py
    â”œâ”€â”€ .env
    â”œâ”€â”€ .gitignore
    â”œâ”€â”€ README.md
    â””â”€â”€ requirements.txt
```

## ğŸ§ª Get Started

### ğŸ”§ Setup & Environment

1.  **Clone the repository**:

    ```bash
    git clone https://github.com/rooniac/nyc-taxi-data-engineering.git
    cd nyc-taxi-data-engineering
    ```

2.  **Install dependencies**:

    ```bash
    python -m venv venv
    source venv/bin/activate  # Windows: venv\Scripts\activate
    pip install -r requirements.txt
    ```

3.  **Setup `.env` file**:
    Create `.env` base on your infomation that you set in docker (PostgreSQL, Minio, Kafka, ...)

### âš™ï¸ Batch Processing

1.  **Download TLC parquet files**:

    You should change variables in `data/download_data.py` to download any month or year data that you want to test.

    ```bash
    python data/download_data.py
    ```

2.  **Run MinIO with docker**:

    ```bash
    docker compose -f docker/docker-compose.yaml up -d
    ```

    Now you can access `localhost:9001` and login to MinIO with your information in docker file.

3.  **Upload raw data to MinIO**:

    ```bash
    python pipelines/ingest/ingest_to_raw.py
    ```

4.  **Run batch transformation**:

    ```bash
    spark-submit pipelines/transform/transform_data.py
    ```

5.  **Convert to Delta format (Optional)** :

    ```bash
    spark-submit pipelines/delta/convert_to_delta.py
    ```

6.  **Run PostgreSQL with docker** :

    ```bash
    docker compose -f docker/postgres-docker-compose.yaml up -d
    ```

7.  **Create schemas and tables** :

    ```bash
    python scripts/create_schema_postgres.py
    python scripts/create_staging_table_pg.py
    ```

8.  **Load to PostgreSQL (staging)** :

    ```bash
    spark-submit pipelines/datalake_to_dw/processed_to_staging.py
    ```

### ğŸ“¡ Streaming Processing

1.  **Start Kafka + Zookeeper + Debezium CDC with docker**:

    ```bash
    docker-compose -f docker/stream-docker-compose.yaml up -d
    ```

2.  **Create CDC source table and insert mock data**:

    ```bash
    python scripts/create_cdcsource_table_pg.py
    python scripts/insert_to_cdc_source.py
    ```

3.  **Register CDC connect with PostgreSQL cdc_source**:

    ```bash
    curl -X POST http://localhost:8083/connectors ^
    -H "Content-Type: application/json" ^
    -d "@debezium\configs\taxi_nyc_connector.json"
    ```

4.  **Start Spark Streaming job**:

    ```bash
    spark-submit streaming_processing/streaming_to_datalake.py
    ```

    After this, you can check raw/streaming in MinIO to see streaming data.

## ğŸ” dbt for Data Warehouse

The final stage of the batch pipeline leverages **dbt (Data Build Tool)** to transform data in the `staging` schema of PostgreSQL into a clean, analytical **star-schema** model stored in the `production` schema.

1.  **Initialize dbt** (only required the first time):

    ```bash
    cd dbt_tripdata
    dbt deps
    ```

2.  **Run all dbt models and transformations:**:

    ```bash
    dbt run
    ```

    After the transformation completes successfully, you can connect to the PostgreSQL instance and query tables in the production schema. These tables are fully modeled and ready for analysis or visualization.

<p align="center">
  <img src="./images/production_star_schema.png" alt="Production Schema" width="900"/>
  <br/>
  <em>Production star schema</em>
</p>

## â° Airflow DAGs Orchestration

1.  **Start Airflow with Docker**:

    You have to keep MinIO, PostgreSQL docker running to use Airflow DAGs.

    ```bash
    docker-compose -f docker/airflow-docker-compose.yaml up -d
    ```

2.  **Airflow UI**:

    Once airflow is running, you can access `localhost:8080` and login with your information. After that, you have to register connections with MinIO and PostgreSQL.

3.  **Trigger DAGs**:

    In this project, the entire pipeline is orchestrated using **five modular Airflow DAGs**, each representing a critical stage in the data lifecycle. Together, they cover the full batch pipeline from raw ingestion to production-ready warehouse tables:

    - **DAG 1 â€“ `nyc_taxi_ingest`**: Downloads NYC TLC data and uploads it to the raw zone in MinIO.
    - **DAG 2 â€“ `nyc_taxi_transform`**: Cleans and standardizes raw data, writing to the processed zone.
    - **DAG 3 â€“ `nyc_taxi_convert_delta`**: Converts processed Parquet data into Delta Lake format for sandbox querying (optional).
    - **DAG 4 â€“ `nyc_taxi_staging`**: Loads processed data into the staging schema of the PostgreSQL warehouse.
    - **DAG 5 â€“ `nyc_taxi_production`**: Runs dbt models to transform staging tables into a star-schema production layer.

    You can trigger these DAGs in sequence to execute the full batch pipeline and observe how data flows through each stage of the architecture.

![Superset Dashboard](./images/airflow_dags.png)

<p align="center">
  <em>Airflow DAGs</em>
</p>

## ğŸ“Š BI Visualization with Superset

If you would like to see the outcome of pipeline, you can access to PostgreSQL and query `production`. However, if you want to visualize the final data we got at `production`, you can use any BI tool you want. In this project, I use Apache Superset to visualize sample data - the first 10 days of 2025 Feb.

![Superset Dashboard](./images/dashboard.png)

<p align="center">
  <em>Superset Dashboard for sample data from 01/02/2025 to 10/02/2025</em>
</p>

## ğŸ“š References

- **NYC TLC Trip Record Data**  
  Open dataset provided by the New York City Taxi and Limousine Commission (TLC), containing detailed trip records for Yellow and Green taxis.  
  ğŸ“ Source: [TLC Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)
